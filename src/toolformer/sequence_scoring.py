import torch

def get_scores_for_labels(input, labels, model, tokenizer):
  # https://colab.research.google.com/drive/1Q8VAwCPB12ZzYH79nAuiiSSnVDkZ2-u7?usp=sharing#scrollTo=WJshYeFQ_IeB

  batch_size, num_labels = len(input), len(labels)
  # Get encodings
  input_enc = tokenizer.batch_encode_plus(input, return_tensors="pt",  add_special_tokens=True, truncation=True, padding="longest")
  target_enc = tokenizer.batch_encode_plus(labels, return_tensors="pt", padding="longest").input_ids

  # Get encoder's last hidden state
  encoder_hidden_states = model.encoder(**input_enc)[0]

  # Repeat the inputs `num_label` times
  encoder_hidden_states = encoder_hidden_states.unsqueeze(dim=1).repeat(1, num_labels, 1, 1).flatten(0, 1)
  attention_mask = input_enc.attention_mask.unsqueeze(dim=1).repeat(1, num_labels, 1).flatten(0, 1)

  # Create the decoding mask (that is commonly generated by the T5 model at predict time) -- makes it more efficient
  decoder_input_ids = torch.cat([torch.zeros((num_labels * batch_size, 1), dtype=torch.int), target_enc[:, :-1].repeat(num_labels, 1)], dim=1)
  decoder_attention_mask = (decoder_input_ids == decoder_input_ids).float()
  lm_target = target_enc - 100 * (target_enc == tokenizer.pad_token_id).long()

  model_output = model(
        attention_mask=attention_mask,
        encoder_outputs=[encoder_hidden_states],
        decoder_input_ids=decoder_input_ids,
        decoder_attention_mask=decoder_attention_mask,
    )

  # Compute the log probabilities associated with each of the labels
  labels_log_probs = F.cross_entropy(
      model_output.logits.flatten(0, 1),
      lm_target.repeat(num_labels, 1).flatten(0, 1),
      reduction="none",
  )

  # Sum log probs for each of the (input, label) pair
  labels_scores = labels_log_probs.view(batch_size, num_labels, -1)
  labels_scores = labels_scores.sum(dim=-1)

  # Note: Label log probabilities are positive (due to the internals of pytorch's
  # cross entropy). To obtain the "logits", we need to multiply by -1.
  return labels_scores * -1